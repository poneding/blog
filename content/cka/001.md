[ğŸ  é¦–é¡µ](../_index.md) / [CKA](_index.md) / 001

# 001

## 01 Task - è‹±æ–‡

Create a new ClusterRole named deployment-clusterrole that only allows the creation of the following resource types:

- Deployment
- StatefulSet
- DaemonSet
  Create a new ServiceAccount named cicd-token in the existing namespace app-team1.
  Limited to namespace app-team1, bind the new ClusterRole deployment-clusterrole to the new ServiceAccount cicd-token.

```bash
kubectl create ns app-team1
kubectl create serviceaccount cicd-token -n app-team1
kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployment,statefulset,daemonset

#limted to the namespace app-team1ã€‚éœ€è¦é™åˆ¶çš„æ˜¯namespaceçº§åˆ«ï¼Œclusterrolebindingä¸ºè®¾ç½®å…¨å±€ï¼Œrolebindingæ­£ç¡®
kubectl create rolebinding cicd-clusterrole -n app-team1 --clusterrole=deployment-clusterrole --serviceaccount=app-team1:cicd-token
```

## 02 Task - è‹±æ–‡

Set the node named ek8s-node-1 as unavaliable and reschedule all the pods running on it.

```bash
kubectl cordon ek8s-node-1
kubectl drain ek8s-node-1 --delete-local-data --ignore-daemonsets --force

//åˆ é™¤æ‰€æœ‰podï¼ˆåŒ…æ‹¬daemonsetç®¡ç†çš„podï¼‰ï¼Œåˆ™éœ€è¦--ignore-daemonsetsæˆ–--ignore-daemonsets=true
```

## 03 Task - è‹±æ–‡

Given an existing Kubernetes cluster running version 1.18.8ï¼Œupgrade all of Kubernetes control plane and node components **on the master node only** to version 1.19.0ã€‚

You are also expected to upgrade kubelet and kubectl on the master nodeã€‚

> Be sure to drain the master node
> before upgrading it and uncordon it after the upgrade.
> Do not upgrade the worker nodes,etcd,the container manager,the CNI plugin,the DNS service or any other addons.

```bash
apt update
apt-cache policy kubeadm
apt-get update && apt-get install -y --allow-change-held-packages kubeadm=1.19.0
kubeadm version #æ£€æŸ¥kubeadmç‰ˆæœ¬
kubectl drain master --ignore-daemonsets --delete-local-data --force #è…¾ç©ºæ§åˆ¶å¹³é¢èŠ‚ç‚¹
sudo kubeadm upgrade plan # å‘½ä»¤æŸ¥çœ‹å¯å‡çº§çš„ç‰ˆæœ¬ä¿¡æ¯
sudo kubeadm upgrade apply v1.19.0 --etcd-upgrade=false #æŸ¥çœ‹ç‰ˆæœ¬ä¿¡æ¯æ—¶ï¼Œæ’é™¤etcdä»3.4.3-0å‡åˆ°3.4.7-0
kubectl uncordon master
sudo kubeadm upgrade node #å‡çº§å…¶ä»–æ§åˆ¶é¢èŠ‚ç‚¹
apt-get update && apt-get install -y --allow-change-held-packages kubelet=1.19.0 kubectl=1.19.0
#å‡çº§å…¶ä»–æ§åˆ¶é¢èŠ‚ç‚¹
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

## 04 Task - ä¸­æ–‡

é¦–å…ˆï¼Œä¸ºè¿è¡Œåœ¨ <https://127.0.0.1:2379> ä¸Šçš„ç°æœ‰etcd å®ä¾‹åˆ›å»ºå¿«ç…§å¹¶å°†å¿«ç…§ä¿å­˜åˆ°/data/backup/etcd-snapshot.dbã€‚

> ä¸ºç»™å®šå®ä¾‹åˆ›å»ºå¿«ç…§é¢„è®¡èƒ½åœ¨å‡ ç§’é’Ÿå†…å®Œæˆã€‚å¦‚æœè¯¥æ“ä½œä¼¼ä¹æŒ‚èµ·ï¼Œåˆ™å‘½ä»¤å¯èƒ½æœ‰é—®é¢˜ã€‚ç”¨ctrl+c æ¥å–æ¶ˆæ“ä½œï¼Œç„¶åé‡è¯•ã€‚

ç„¶åè¿˜åŸä½äº/var/data/etcd-snapshot-previous.dbçš„ç°æœ‰å…ˆå‰å¿«ç…§ã€‚

> æä¾›äº†ä»¥ä¸‹TLSè¯ä¹¦å’Œå¯†é’¥ï¼Œä»¥é€šè¿‡etcdctlè¿æ¥åˆ°æœåŠ¡å™¨ã€‚
>
> - caè¯ä¹¦ï¼š/opt/KUIN00601/ca.crt
> - å®¢æˆ·ç«¯è¯ä¹¦ï¼š/opt/KUIN00601/etcd-client.crt
> - å®¢æˆ·ç«¯å¯†é’¥ï¼š/opt/KUIN00601/etcd-client.key

ä¸€å®šè¦æŠŠè¿™å‚æ•°ç”¨ç†Ÿç»ƒï¼Œå¦‚æœè€ƒè¯•æ—¶æœ‰é—®é¢˜ï¼Œä¸è¦æ€¥ï¼Œå¤šè¯•è¯•ï¼ï¼ï¼

> ä¸€æ—¦æ­£ç¡®é…ç½®äº† etcdï¼Œåªæœ‰å…·æœ‰æœ‰æ•ˆè¯ä¹¦çš„å®¢æˆ·ç«¯æ‰èƒ½è®¿é—®å®ƒã€‚è¦è®© Kubernetes API æœåŠ¡å™¨è®¿é—®ï¼Œå¯ä»¥ä½¿ç”¨å‚æ•° --etcd-certfile=k8sclient.cert,â€“etcd-keyfile=k8sclient.key å’Œ --etcd-cafile=ca.cert é…ç½®å®ƒã€‚

æˆ‘è®°å¾—æˆ‘è€ƒè¯•è¿›ç”¨çš„æ˜¯ï¼šâ€“certfile=/opt/KUIN00601/etcd-client.crt --keyfile=/opt/KUIN00601/etcd-client.key --cafile=/opt/KUIN00601/ca.crt

```bash
ETCDCTL_API=3 etcdctl --endpoint=https://127.0.0.1:2379 --certfile=/opt/KUIN00601/etcd-client.crt --keyfile=/opt/KUIN00601/etcd-client.key --cafile=/opt/KUIN00601/ca.crt snapshot save /data/backup/etcd-snapshot.db

ETCDCTL_API=3 etcdctl --endpoint=https://127.0.0.1:2379 --cert-file=/opt/KUIN00601/etcd-client.crt --key-file=/opt/KUIN00601/etcd-client.key --ca-file=/opt/KUIN00601/ca.crt snapshot restore /var/data/etcd-snapshot-previous.db
```

## 05 Task - è‹±æ–‡

Create a new **NetworkPolicy** named **allow-port-from-namespace** to allow Pods in the existing namespace **internal** to connect to port **8080** of other Pods in the same namespace.
Ensure that the new NetworkPolicy:

- does **not** allow access to Pods not listening on port **8080**.
- does **not** allow access from Pods not in namespace **internal**.

```bash
#network.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-port-from-namespace
  namespace: internal
spec:
  podSelector: 
    matchLabels: {}
  policyTypes:
  - Ingress
  ingress:
  - from: 
    - podSelector: {} 
  ports: 
  - protocol: TCP 
    port: 8080
#spec.podSelectoré™å®šäº†è¿™ä¸ªnamespaceé‡Œçš„podå¯ä»¥è®¿é—®
kubectl create -f network.yaml
```

## 06 Task - è‹±æ–‡

Reconfigure the existing deployment **front-end** and add a port specifiction named **http** exposing port **80/tcp** of the existing container **nginx**.

Create a new service named **front-end-svc** exposing the container prot **http**.

Configure the new service to also expose the individual Pods via a NodePort on the nodes on which they are scheduled.

```bash
kubectl get deploy front-end
kubectl edit deploy front-end -o yaml
#port specification named http
#service.yaml
apiVersion: v1
kind: Service
metadata:
  name: front-end-svc
  labels: app: nginx
spec:
  ports:
  - port: 80 protocol: tcp name: http
  selector: app: nginx
  type: NodePort  
#
kubectl create -f service.yaml
#
kubectl get svc

#æˆ–è€…ä¸€æ¡å‘½ä»¤æå®š,æ³¨æ„ä¼šé—æ¼port specification named http
kubectl expose deployment front-end --name=front-end-svc --port=80 --tarport=80 --type=NodePort
```

## 07 Task - è‹±æ–‡

Create a new nginx Ingress resource as follows:

- Name: **ping**ï¼š
- Namespace: **ing-internal**ï¼š
- Exposing service **hi** on path **/hi** using service port **5678**ï¼š

> The avaliability of service **hi** can be checked using the following command,which should return **hi**:
> curl -kL /hi

```bash
vi ingress.yaml
#
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ping
  namespace: ing-internal
spec:
  rules:
  - http: paths: - path: /hi pathType: Prefix backend: service: name: hi port: number: 5678
# 
kubectl create -f ingress.yaml
```

## 08 Task - è‹±æ–‡

Scale the deployment **presentation** to **3** pods.

```bash
kubectl get deployment
kubectl scale deployment.apps/presentation --replicas=3
```

## 09 Task - è‹±æ–‡

Task

Schedule a pod as follows:

- name: **nginx-kusc00401**ï¼š
- Image: **nginx**ï¼š
- Node selector: **disk-spinning**ï¼š

```bash
#yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-kusc00401
spec:
  containers:
  - name: nginx image: nginx imagePullPolicy: IfNotPresent
  nodeSelector: disk: spinning
# 
kubectl create -f node-select.yaml 
```

## 10 Task - è‹±æ–‡

Task
Check to see how many nodes are ready (not including nodes tainted **NoSchedule**)and write the number to **/opt/KUSC00402/kusc00402.txt.**ï¼š

```bash
kubectl describe nodes | grep ready|wc -l
kubectl describe nodes | grep -i taint | grep -i noschedule |wc -l
echo 3 > /opt/KUSC00402/kusc00402.txt

# æŸ¥è¯¢é›†ç¾¤ReadyèŠ‚ç‚¹æ•°é‡
kubectl get node | grep -i ready |wc -l
# æ‰¾å‡ºèŠ‚ç‚¹taintsã€noSchedule
kubectl describe nodes | grep -i taints | grep -i noschedule |wc -l
#å°†å¾—åˆ°çš„å‡æ•°ï¼Œå†™å…¥åˆ°æ–‡ä»¶
echo 2 > /opt/KUSC00402/kusc00402.txt
```

## 11 Task - è‹±æ–‡

Create a pod named **kucc8** with a single app container for each of the following images running inside (there may be between 1 and 4 images specified):
**nginx + redis + memcached + consul .**ï¼š

```bash
kubectl run kucc8 --image=nginx --dry-run -o yaml > kucc8.yaml
# vi kucc8.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  name: kucc8
spec:
  containers:
  - image: nginx name: nginx
  - image: redis name: redis
  - image: memcached name: memcached
  - image: consul name: consul
# 
kubectl create -f kucc8.yaml
#12.07
```

## 12 Task - è‹±æ–‡

Task
Create a persistent volume whit name **app-config**, of capacity **1Gi** and access mode ReadOnlyMany . the type of volume is **hostPath** and its location is **/srv/app-config .**ï¼š

```bash
#vi pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: app-config
spec:
  capacity: storage: 1Gi
  accessModes: - ReadOnlyMany
  hostPath: path: /srv/app-config
#
kubectl create -f pv.yaml
```

## 13 Task - è‹±æ–‡

Task
Create a new **PersistentVolumeClaim**:

- Name: **pv-volume**ï¼š
- Class: **csi-hostpath-sc**ï¼š
- Capacity: **10Mi**ï¼š

Create a new Pod which mounts the **PersistentVolumeClaim** as a volume:

- Name: **web-server**ï¼š
- Image: **nginx**ï¼š
- Mount path: **/usr/share/nginx/html**ï¼š

Configure the new Pod to have **ReadWriteOnce** access on the volume.

Finally,using **kubectl edit** or **Kubectl patch** expand the **PersistentVolumeClaim** to a capacity of **70Mi** and record that change.

```bash
vi pvc.yaml
#ä½¿ç”¨æŒ‡å®šstorageclassåˆ›å»ºä¸€ä¸ªpvc
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pv-volume
spec:
  accessModes: - ReadWriteOnce
  volumeMode: Filesystem resources: requests: storage: 10Mi
  storageClassName: csi-hostpath-sc

# vi pod-pvc.yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-server
spec:
  containers: - name: web-server image: nginx volumeMounts: - mountPath: "/usr/share/nginx/html" name: my-volume
  volumes: - name: my-volume persistentVolumeClaim: claimName: pv-volume
# craete 
kubectl create -f pod-pvc.yaml
#edit ä¿®æ”¹å®¹é‡
kubectl edit pvc pv-volume --record
```

## 14 Task - è‹±æ–‡

Task
Monitor the logs of pod **bar** and:

- Extract log lines corresponding to error **unable-to-access-website**ï¼š
- Write them to **/opt/KUTR00101/bar**ï¼š

```bash
kubectl logs bar | grep 'unable-to-access-website' > /opt/KUTR00101/bar
cat /opt/KUTR00101/bar
```

## 15 Task - è‹±æ–‡

Context
Without changing its existing containers,an existing Pod needs to be integrated into Kubernetesâ€™s build-in logging architecture (e.g. kubectl logs). Adding a streaming sidecar container is a good and common way to accomplish this requirement.

Task
Add a **busybox** sidecar container to the existing Pod **big-corp-app**. The new sidecar container has to run the following command:

```bash
/bin/sh -c tail -n+1 -f /var/log/big-corp-app.log
```

Use a volume mount named **logs** to make the file **/var/log/big-corp-app.log** available to the sidecar container.

> Donâ€™t modify the existing container.
> Donâ€™t modify the path of the log file,both containers must access it at **/var/log/big-corp-app.log**.

```bash
#
kubectl get pod big-corp-app -o yaml 
#
apiVersion: v1
kind: Pod
metadata:
  name: big-corp-app
spec:
  containers:
  - name: big-corp-app image: busybox args: - /bin/sh - -c - > i=0; while true; do echo "$(date) INFO $i" >> /var/log/big-corp-app.log; i=$((i+1)); sleep 1; done volumeMounts: - name: logs mountPath: /var/log
  - name: count-log-1 image: busybox args: [/bin/sh, -c, 'tail -n+1 -f /var/log/big-corp-app.log'] volumeMounts: - name: logs mountPath: /var/log
  volumes:
  - name: logs emptyDir: {}

#éªŒè¯ï¼š
kubectl logs big-corp-app -c count-log-1
```

## 16 Task - è‹±æ–‡

Form the pod label **name-cpu-loader**,find pods running high CPU workloads and write the name of the pod consuming most CPU to the file **/opt/KUTR00401/KURT00401.txt**(which alredy exists).

æŸ¥çœ‹Podæ ‡ç­¾ä¸ºname=cpu-user-loader çš„CPUä½¿ç”¨ç‡å¹¶ä¸”æŠŠcpuä½¿ç”¨ç‡æœ€é«˜çš„podåç§°å†™å…¥/opt/KUTR00401/KUTR00401.txtæ–‡ä»¶é‡Œ

```bash
kubectl top pods -l name=name-cpu-loader --sort-by=cpu
echo 'æ’åç¬¬ä¸€çš„podåç§°' >>/opt/KUTR00401/KUTR00401.txt
```

## 17 Task - è‹±æ–‡

Task
A Kubernetes worker node,named **wk8s-node-0** is in state **NotReady** .
Investigate why this is the case,and perform any appropriate steps to bring the node to a **Ready** state,ensuring that any changes are made permanent.

> Yon can **ssh** to teh failed node using:
>
> ```bash
> ssh wk8s-node-o
> ```
>
> You can assume elevated privileges on the node with the following command:
>
> ```bash
> sudo -i
> ```

```bash
#åä¸ºwk8s-node-1 çš„èŠ‚ç‚¹å¤„äºNotReadyçŠ¶æ€ï¼Œå°†å…¶æ¢å¤æˆReadyçŠ¶æ€ï¼Œå¹¶ä¸”è®¾ç½®ä¸ºå¼€æœºè‡ªå¯
# è¿æ¥åˆ°NotReadyèŠ‚ç‚¹
ssh wk8s-node-0
#è·å–æƒé™
sudo -i
# æŸ¥çœ‹æœåŠ¡æ˜¯å¦è¿è¡Œæ­£å¸¸
systemctl status kubelet 
#å¦‚æœæœåŠ¡éæ­£å¸¸è¿è¡Œè¿›è¡Œæ¢å¤
systemctl start kubelet
#è®¾ç½®å¼€æœºè‡ªå¯
systemctl enable kubelet 
```

## 19 Task - è‹±æ–‡

Set configuration context $ kubectl config use-context wk8s

configure the kubelet systemed managed service, on the node labelled with name=wk8s-node-1,to launch a pod containing a single container of image nginx named myservice automatically.

Any spec file requried should be placed in the /etc/kuberneteds/mainfests directory on the node

Hints:

You can ssh to the failed node using $ ssh wk8s-node-0

You can assume elevated privileges on the node with the following command $ sudo -i

## é™æ€Podåˆ›å»ºæ–¹æ³•ä¸æ³¨æ„ç‚¹

Set configuration context $ kubectl config use-context wk8s

configure the kubelet systemed managed service, on the node labelled with name=wk8s-node-1,to launch a pod containing a single container of image nginx named myservice automatically.

Any spec file requried should be placed in the /etc/kuberneteds/mainfests directory on the node

Hints:

You can ssh to the failed node using $ ssh wk8s-node-0

You can assume elevated privileges on the node with the following command $ sudo -i

```bash
kubectl config use-context wk8s
kubectl get node -l=name=wk8s-node-0 -o wide
# or
kubectl get node -l name=wk8s-node-0 -o wide
sudo wk8s-node-0
sudo -i
systemctl status kubelet -l |grep config #æ‰¾åˆ°--configé…ç½®çš„æ–‡ä»¶è·¯å¾„
cat /var/lib/kubelet/config.yaml |grep staticPodPath # å¾—åˆ°/etc/kubernetes/manifests
cd /etc/kubernetes/manifests
kubectl run myservice --image=nginx --dry-run=client -o yaml > myservice.yaml
kubectl get pod -A|grep myservice #å¯ä»¥å¾—åˆ°é™æ€Pod 
```

---
[Â» å‡†å¤‡CKA](prepare-cka.md)
